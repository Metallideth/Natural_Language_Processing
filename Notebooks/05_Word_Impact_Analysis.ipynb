{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math\n",
    "from openpyxl import Workbook\n",
    "import xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../inference/24-02-2024_1200_impact_output.pkl','rb') as file:\n",
    "    impact_output = pickle.load(file)\n",
    "with open('../Data/index_label_mapping.pkl','rb') as file:\n",
    "    index_label_mapping = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669785"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(impact_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Role': ['GOVERNANCE RISK COMPLIANCE',\n",
       "  'INFORMATION SECURITY',\n",
       "  'IT GENERAL',\n",
       "  'NETWORKING',\n",
       "  'NON-ICP',\n",
       "  'SYSTEMS'],\n",
       " 'Function': ['ENGINEERING',\n",
       "  'IT',\n",
       "  'NON-ICP',\n",
       "  'PROCUREMENT',\n",
       "  'RISK/LEGAL/COMPLIANCE'],\n",
       " 'Level': ['C-LEVEL',\n",
       "  'CONTRIBUTOR',\n",
       "  'DIRECTOR',\n",
       "  'EXECUTIVE',\n",
       "  'MANAGER',\n",
       "  'UNKNOWN']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values = {x.replace(\"Job \",\"\"):list(index_label_mapping[x].values()) for x in index_label_mapping}\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sequence': 'IT DIRECTOR',\n",
       " 'Role': {'Prediction': 'NETWORKING',\n",
       "  'Target': 'NETWORKING',\n",
       "  'Correct?': True,\n",
       "  'Distinct_Tokens': ['IT', 'DIRECTOR'],\n",
       "  'Token_Importance': {'IT': 0.408794105052948, 'DIRECTOR': 0.591205894947052},\n",
       "  'Token_Rank': {'IT': 2, 'DIRECTOR': 1},\n",
       "  'Token_Marginal_Score_Positive': {'IT': 8.886344909667969,\n",
       "   'DIRECTOR': 12.851602554321289},\n",
       "  'Token_Marginal_Score_Raw': {'IT': 8.886344909667969,\n",
       "   'DIRECTOR': 12.851602554321289}},\n",
       " 'Function': {'Prediction': 'IT',\n",
       "  'Target': 'IT',\n",
       "  'Correct?': True,\n",
       "  'Distinct_Tokens': ['IT', 'DIRECTOR'],\n",
       "  'Token_Importance': {'IT': 0.6040630340576172,\n",
       "   'DIRECTOR': 0.3959369659423828},\n",
       "  'Token_Rank': {'IT': 1, 'DIRECTOR': 2},\n",
       "  'Token_Marginal_Score_Positive': {'IT': 11.600852966308594,\n",
       "   'DIRECTOR': 7.603853225708008},\n",
       "  'Token_Marginal_Score_Raw': {'IT': 11.600852966308594,\n",
       "   'DIRECTOR': 7.603853225708008}},\n",
       " 'Level': {'Prediction': 'DIRECTOR',\n",
       "  'Target': 'DIRECTOR',\n",
       "  'Correct?': True,\n",
       "  'Distinct_Tokens': ['IT', 'DIRECTOR'],\n",
       "  'Token_Importance': {'IT': 0.23911701142787933,\n",
       "   'DIRECTOR': 0.7608829736709595},\n",
       "  'Token_Rank': {'IT': 2, 'DIRECTOR': 1},\n",
       "  'Token_Marginal_Score_Positive': {'IT': 3.7588276863098145,\n",
       "   'DIRECTOR': 11.960787773132324},\n",
       "  'Token_Marginal_Score_Raw': {'IT': 3.7588276863098145,\n",
       "   'DIRECTOR': 11.960787773132324}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impact_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize compilation dictionary - we'll have entries for role, function, and level, which will lead to\n",
    "# entries for each potential output category, which will lead to entries for every individual word with:\n",
    "# 1. Average token importance\n",
    "# 2. Average number of unique tokens in sequence\n",
    "# 3. Average marginal score (positive) - keyword\n",
    "# 4. Average marginal score (raw)\n",
    "# 5. Average marginal score (negative) - anti-keyword\n",
    "# 6. Average token score rank\n",
    "# For 1-6 we'll first need to just record every entry in the structure, then we can run through and take the average\n",
    "keyword_dict_running = {**unique_values}\n",
    "for x in keyword_dict_running:\n",
    "    keyword_dict_running[x] = {}\n",
    "    for y in unique_values[x]:\n",
    "        keyword_dict_running[x][y] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/669785 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 669785/669785 [00:33<00:00, 19869.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run through the data\n",
    "# tqdm(enumerate(data_loader,0),total=len(data_loader))\n",
    "for _,sequence in tqdm(enumerate(impact_output,0),total=len(impact_output)):\n",
    "    for key in keyword_dict_running:\n",
    "        this_sequence_info = sequence[key]\n",
    "        this_prediction = this_sequence_info['Prediction']\n",
    "        this_tokens = this_sequence_info['Distinct_Tokens']\n",
    "        this_unique_tokens_count = len(this_tokens)\n",
    "        for token in this_tokens:\n",
    "            # If not already present in the keyword_dict_running, add it\n",
    "            if token not in keyword_dict_running[key][this_prediction]:\n",
    "                keyword_dict_running[key][this_prediction][token] = defaultdict(list)\n",
    "            # Append to lists the abovementioned metrics\n",
    "            token_importance = this_sequence_info['Token_Importance'][token]\n",
    "            marginal_score_positive = this_sequence_info['Token_Marginal_Score_Positive'][token]\n",
    "            raw_score = this_sequence_info['Token_Marginal_Score_Raw'][token]\n",
    "            token_rank = this_sequence_info['Token_Rank'][token] \n",
    "            if math.isnan(token_importance):\n",
    "                token_importance = 0\n",
    "            keyword_dict_running[key][this_prediction][token]['Token_Importance'].append(token_importance)\n",
    "            keyword_dict_running[key][this_prediction][token]['Unique_Tokens_Count'].append(this_unique_tokens_count)\n",
    "            keyword_dict_running[key][this_prediction][token]['Marginal_Score_Positive'].append(marginal_score_positive)\n",
    "            keyword_dict_running[key][this_prediction][token]['Marginal_Score_Raw'].append(raw_score)\n",
    "            keyword_dict_running[key][this_prediction][token]['Marginal_Score_Negative'].append(min(raw_score,0))\n",
    "            keyword_dict_running[key][this_prediction][token]['Token_Rank'].append(token_rank)\n",
    "            keyword_dict_running[key][this_prediction][token]['Token_Occurrences'].append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dict_average = {**unique_values}\n",
    "for x in keyword_dict_average:\n",
    "    keyword_dict_average[x] = {}\n",
    "    for y in unique_values[x]:\n",
    "        keyword_dict_average[x][y] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1706it [00:00, 18196.92it/s]\n",
      "9556it [00:00, 12284.14it/s]\n",
      "4083it [00:00, 21315.79it/s]\n",
      "8604it [00:00, 11714.27it/s]\n",
      "13456it [00:00, 16285.65it/s]\n",
      "2293it [00:00, 23263.14it/s]\n",
      "4683it [00:00, 18333.95it/s]\n",
      "17497it [00:01, 9487.11it/s] \n",
      "10285it [00:00, 20702.84it/s]\n",
      "602it [00:00, 22362.89it/s]\n",
      "533it [00:00, 26644.63it/s]\n",
      "5048it [00:00, 16681.92it/s]\n",
      "11147it [00:00, 14593.11it/s]\n",
      "7688it [00:00, 14873.74it/s]\n",
      "7318it [00:00, 16439.17it/s]\n",
      "8310it [00:00, 13781.54it/s]\n",
      "1407it [00:00, 29829.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we run through the above dictionary and calculate the averages for each token and underlying list\n",
    "for key in keyword_dict_running:\n",
    "    for prediction in keyword_dict_running[key]:\n",
    "        keyword_dict_average[key][prediction] = defaultdict(list)\n",
    "        for _,token in tqdm(enumerate(keyword_dict_running[key][prediction])):\n",
    "            keyword_dict_average[key][prediction]['Token'].append(token)\n",
    "            keyword_dict_average[key][prediction]['Avg_Token_Importance'].append(np.array(keyword_dict_running[key][prediction][token]['Token_Importance']).mean())\n",
    "            keyword_dict_average[key][prediction]['Avg_Unique_Tokens_Count'].append(np.array(keyword_dict_running[key][prediction][token]['Unique_Tokens_Count']).mean())\n",
    "            keyword_dict_average[key][prediction]['Avg_Marginal_Score_Positive'].append(np.array(keyword_dict_running[key][prediction][token]['Marginal_Score_Positive']).mean())\n",
    "            keyword_dict_average[key][prediction]['Avg_Marginal_Score_Raw'].append(np.array(keyword_dict_running[key][prediction][token]['Marginal_Score_Raw']).mean())\n",
    "            keyword_dict_average[key][prediction]['Avg_Marginal_Score_Negative'].append(np.array(keyword_dict_running[key][prediction][token]['Marginal_Score_Negative']).mean())\n",
    "            keyword_dict_average[key][prediction]['Avg_Token_Rank'].append(np.array(keyword_dict_running[key][prediction][token]['Token_Rank']).mean())\n",
    "            keyword_dict_average[key][prediction]['Total_Token_Occurrences'].append(np.array(keyword_dict_running[key][prediction][token]['Token_Occurrences']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a table for each output and possible value for that output. Let's start with a new dictionary to house it\n",
    "keyword_table_dict = {}\n",
    "for x in unique_values:\n",
    "    keyword_table_dict[x] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keyword_dict_average:\n",
    "    for output in keyword_dict_average[key]:\n",
    "        keyword_table_dict[key][output] = pd.DataFrame.from_dict(keyword_dict_average[key][output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write tables to excel - filter out values with less than 100 total token occurrences\n",
    "writer_keyword = pd.ExcelWriter('../inference/impact_output_keyword.xlsx',engine = 'xlsxwriter')\n",
    "for key in keyword_table_dict:\n",
    "    for output in keyword_table_dict[key]:\n",
    "        this_df = keyword_table_dict[key][output] \n",
    "        this_df[this_df.Total_Token_Occurrences >= 100].sort_values(by = 'Avg_Token_Importance',ascending=False).to_excel(\n",
    "            writer_keyword,sheet_name='{}_{}'.format(key.replace('/',''),output.replace('/','')),index = False)\n",
    "writer_keyword.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netskope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
