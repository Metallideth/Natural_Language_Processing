{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../inference/27-02-2024_1413_impact_output.pkl','rb') as file:\n",
    "    impact_output = pickle.load(file)\n",
    "with open('../Data/index_label_mapping.pkl','rb') as file:\n",
    "    index_label_mapping = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "669785"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(impact_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Role': ['DEVELOPMENT',\n",
       "  'GOVERNANCE RISK COMPLIANCE',\n",
       "  'INFORMATION SECURITY',\n",
       "  'IT GENERAL',\n",
       "  'NETWORKING',\n",
       "  'NON-ICP',\n",
       "  'SYSTEMS'],\n",
       " 'Function': ['ENGINEERING',\n",
       "  'IT',\n",
       "  'NON-ICP',\n",
       "  'PROCUREMENT',\n",
       "  'RISK/LEGAL/COMPLIANCE'],\n",
       " 'Level': ['C-LEVEL',\n",
       "  'CONTRIBUTOR',\n",
       "  'DIRECTOR',\n",
       "  'EXECUTIVE',\n",
       "  'MANAGER',\n",
       "  'UNKNOWN']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_values = {x.replace(\"Job \",\"\"):list(index_label_mapping[x].values()) for x in index_label_mapping}\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sequence': 'IT DIRECTOR',\n",
       " 'Role': {'Prediction': 'NETWORKING',\n",
       "  'Target': 'NETWORKING',\n",
       "  'Correct?': True,\n",
       "  'Distinct_Tokens': ['IT', 'DIRECTOR'],\n",
       "  'Token_Importance': {'IT': 0.39951249957084656,\n",
       "   'DIRECTOR': 0.6004875302314758},\n",
       "  'Token_Rank': {'IT': 2, 'DIRECTOR': 1},\n",
       "  'Token_Marginal_Score_Positive': {'IT': 9.991130828857422,\n",
       "   'DIRECTOR': 15.017175674438477},\n",
       "  'Token_Marginal_Score_Raw': {'IT': 9.991130828857422,\n",
       "   'DIRECTOR': 15.017175674438477}},\n",
       " 'Function': {'Prediction': 'IT',\n",
       "  'Target': 'IT',\n",
       "  'Correct?': True,\n",
       "  'Distinct_Tokens': ['IT', 'DIRECTOR'],\n",
       "  'Token_Importance': {'IT': 0.7296534180641174,\n",
       "   'DIRECTOR': 0.2703465223312378},\n",
       "  'Token_Rank': {'IT': 1, 'DIRECTOR': 2},\n",
       "  'Token_Marginal_Score_Positive': {'IT': 8.235910415649414,\n",
       "   'DIRECTOR': 3.0515170097351074},\n",
       "  'Token_Marginal_Score_Raw': {'IT': 8.235910415649414,\n",
       "   'DIRECTOR': 3.0515170097351074}},\n",
       " 'Level': {'Prediction': 'DIRECTOR',\n",
       "  'Target': 'DIRECTOR',\n",
       "  'Correct?': True,\n",
       "  'Distinct_Tokens': ['IT', 'DIRECTOR'],\n",
       "  'Token_Importance': {'IT': 0.04350265860557556,\n",
       "   'DIRECTOR': 0.956497311592102},\n",
       "  'Token_Rank': {'IT': 2, 'DIRECTOR': 1},\n",
       "  'Token_Marginal_Score_Positive': {'IT': 0.5885601043701172,\n",
       "   'DIRECTOR': 12.940730094909668},\n",
       "  'Token_Marginal_Score_Raw': {'IT': 0.5885601043701172,\n",
       "   'DIRECTOR': 12.940730094909668}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impact_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize compilation dictionary - we'll have entries for role, function, and level, which will lead to\n",
    "# entries for each potential output category, which will lead to entries for every individual word with:\n",
    "# 1. Average token importance\n",
    "# 2. Average number of unique tokens in sequence\n",
    "# 3. Average marginal score (positive) - keyword\n",
    "# 4. Average marginal score (raw)\n",
    "# 5. Average marginal score (negative) - anti-keyword\n",
    "# 6. Average token score rank\n",
    "# For 1-6 we'll first need to just record every entry in the structure, then we can run through and take the average\n",
    "keyword_dict_running = {**unique_values}\n",
    "for x in keyword_dict_running:\n",
    "    keyword_dict_running[x] = {}\n",
    "    for y in unique_values[x]:\n",
    "        keyword_dict_running[x][y] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/669785 [00:01<52:51:06,  3.52it/s]C:\\Users\\csarc\\AppData\\Local\\Temp\\ipykernel_32812\\1449636291.py:20: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  priority_index = marginal_score_positive/average_score_difference\n",
      "100%|██████████| 669785/669785 [01:12<00:00, 9220.85it/s] \n"
     ]
    }
   ],
   "source": [
    "# Run through the data\n",
    "# tqdm(enumerate(data_loader,0),total=len(data_loader))\n",
    "for _,sequence in tqdm(enumerate(impact_output,0),total=len(impact_output)):\n",
    "    for key in keyword_dict_running:\n",
    "        this_sequence_info = sequence[key]\n",
    "        this_prediction = this_sequence_info['Prediction']\n",
    "        this_tokens = this_sequence_info['Distinct_Tokens']\n",
    "        this_unique_tokens_count = len(this_tokens)\n",
    "        total_score_difference = np.array(list(this_sequence_info['Token_Marginal_Score_Positive'].values())).sum()\n",
    "        average_score_difference = total_score_difference/this_unique_tokens_count\n",
    "        for token in this_tokens:\n",
    "            # If not already present in the keyword_dict_running, add it\n",
    "            if token not in keyword_dict_running[key][this_prediction]:\n",
    "                keyword_dict_running[key][this_prediction][token] = defaultdict(list)\n",
    "            # Append to lists the abovementioned metrics\n",
    "            token_importance = this_sequence_info['Token_Importance'][token]\n",
    "            marginal_score_positive = this_sequence_info['Token_Marginal_Score_Positive'][token]\n",
    "            raw_score = this_sequence_info['Token_Marginal_Score_Raw'][token]\n",
    "            token_rank = this_sequence_info['Token_Rank'][token]\n",
    "            priority_index = marginal_score_positive/average_score_difference\n",
    "            if np.isnan(priority_index):\n",
    "                priority_index = 0\n",
    "            excess_ri = max(0,priority_index - 1)\n",
    "            if math.isnan(token_importance):\n",
    "                token_importance = 0\n",
    "            keyword_dict_running[key][this_prediction][token]['Token_Importance'].append(token_importance)\n",
    "            keyword_dict_running[key][this_prediction][token]['Unique_Tokens_Count'].append(this_unique_tokens_count)\n",
    "            keyword_dict_running[key][this_prediction][token]['Marginal_Score_Positive'].append(marginal_score_positive)\n",
    "            keyword_dict_running[key][this_prediction][token]['Marginal_Score_Raw'].append(raw_score)\n",
    "            keyword_dict_running[key][this_prediction][token]['Marginal_Score_Negative'].append(min(raw_score,0))\n",
    "            keyword_dict_running[key][this_prediction][token]['Token_Rank'].append(token_rank)\n",
    "            keyword_dict_running[key][this_prediction][token]['Token_Occurrences'].append(1)\n",
    "            keyword_dict_running[key][this_prediction][token]['Priority_Index_>_1'].append((priority_index > 1)*1)\n",
    "            keyword_dict_running[key][this_prediction][token]['Priority_Index'].append(priority_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_dict_average = {**unique_values}\n",
    "for x in keyword_dict_average:\n",
    "    keyword_dict_average[x] = {}\n",
    "    for y in unique_values[x]:\n",
    "        keyword_dict_average[x][y] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4943it [00:00, 18787.39it/s]\n",
      "1700it [00:00, 21784.73it/s]\n",
      "9566it [00:00, 15315.79it/s]\n",
      "3952it [00:00, 23619.17it/s]\n",
      "8623it [00:00, 15831.83it/s]\n",
      "11093it [00:00, 20965.75it/s]\n",
      "2301it [00:00, 27122.92it/s]\n",
      "4691it [00:00, 20858.56it/s]\n",
      "17506it [00:01, 13352.59it/s]\n",
      "10275it [00:00, 21475.65it/s]\n",
      "619it [00:00, 28532.99it/s]\n",
      "533it [00:00, 30071.75it/s]\n",
      "5040it [00:00, 19998.88it/s]\n",
      "11178it [00:00, 17384.56it/s]\n",
      "7681it [00:00, 18070.75it/s]\n",
      "7324it [00:00, 19388.06it/s]\n",
      "8322it [00:00, 17792.35it/s]\n",
      "1248it [00:00, 27959.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now we run through the above dictionary and calculate the averages for each token and underlying list\n",
    "for key in keyword_dict_running:\n",
    "    for prediction in keyword_dict_running[key]:\n",
    "        keyword_dict_average[key][prediction] = defaultdict(list)\n",
    "        for _,token in tqdm(enumerate(keyword_dict_running[key][prediction])):\n",
    "            priority_index = np.array(keyword_dict_running[key][prediction][token]['Priority_Index'])\n",
    "            token_occurrences = np.array(keyword_dict_running[key][prediction][token]['Token_Occurrences'])\n",
    "            pi_g_1 = np.array(keyword_dict_running[key][prediction][token]['Priority_Index_>_1'])\n",
    "            keyword_dict_average[key][prediction]['Token'].append(token)\n",
    "            keyword_dict_average[key][prediction]['Total_Token_Occurrences'].append(np.array(keyword_dict_running[key][prediction][token]['Token_Occurrences']).sum())\n",
    "            keyword_dict_average[key][prediction]['Prob_Priority_Index_>_1'].append(pi_g_1.sum()/token_occurrences.sum())\n",
    "            keyword_dict_average[key][prediction]['StDev_Priority_Index'].append(np.std(priority_index))\n",
    "            keyword_dict_average[key][prediction]['Avg_Priority_Index'].append(np.mean(priority_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we create a table for each output and possible value for that output. Let's start with a new dictionary to house it\n",
    "keyword_table_dict = {}\n",
    "for x in unique_values:\n",
    "    keyword_table_dict[x] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in keyword_dict_average:\n",
    "    for output in keyword_dict_average[key]:\n",
    "        keyword_table_dict[key][output] = pd.DataFrame.from_dict(keyword_dict_average[key][output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write tables to excel - filter out values with less than 100 total token occurrences\n",
    "writer_keyword = pd.ExcelWriter('../inference/impact_output_keyword_ALTERNATE.xlsx',engine = 'xlsxwriter')\n",
    "for key in keyword_table_dict:\n",
    "    for output in keyword_table_dict[key]:\n",
    "        this_df = keyword_table_dict[key][output] \n",
    "        this_df[(this_df.Total_Token_Occurrences >= 100) & (this_df.Avg_Priority_Index > 1)].sort_values(by = 'Avg_Priority_Index',ascending=False).to_excel(\n",
    "            writer_keyword,sheet_name='{}_{}'.format(key.replace('/',''),output.replace('/','')),index = False)\n",
    "writer_keyword.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netskope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
